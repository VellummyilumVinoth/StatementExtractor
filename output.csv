actualNode,err:Syntax|SyntaxNode actualNode = syntaxNodeFromLines(k, rule, subject);
err:Syntax|SyntaxNode actualNode = syntaxNodeFromLines(k, rule, subject);,SyntaxNode normalizedActualNode = normalizeSyntaxNode(actualNode);
normalizedActualNode,string[] actualNodeLines = syntaxNodeToString(normalizedActualNode);
SyntaxNode normalizedActualNode = normalizeSyntaxNode(actualNode);,SyntaxNode node;
actualNodeLines,SourceFile file = createSourceFile(lines, { filename: k });
string[] actualNodeLines = syntaxNodeToString(normalizedActualNode);,Tokenizer tok = new (file);
node,err:Syntax|Token? t = advance(tok, k, lines);
SyntaxNode node;,d:LineColumn lc = file.lineColumn(tok.currentStartPos());
file,string src = lines[lc[0] - 1];
SourceFile file = createSourceFile(lines, { filename: k });,int tStart = lc[1];
tok,string tStr = tokenToString(t);
Tokenizer tok = new (file);,string srcAtPos = src.substring(tStart, tStart + tStr.length());
t,err:Syntax? e = tok.advance();
err:Syntax|Token? t = advance(tok, k, lines);,map<TokenizerTestCase> all = check invalidTokenSourceFragments();
lc,int invalidCases = all.length();
d:LineColumn lc = file.lineColumn(tok.currentStartPos());,map<ParserTestCase> valid = check readParserTests();
src,SingleStringTokenizerTestCase[] sources = [ ["E", string`"`], ["E", "'"], ["E", "`"], ["E", string`"\"`], ["E", string`"\a"`], ["E", string`\`], ["E", string`"${ "\n" }"`], ["E", string`"${ "\r" }"`], ["E", string`"\\`], ["E", string`"\u{}"`], ["E", "\"\\" + "u{D800}\""], ["E", "\"\\" + "u{DFFF}\""], ["E", "\"\\" + "u{110000}\""], ["E", string`"\u{X}"`], ["E", string`"\u{-6A}"`], ["E", string`"\u"`], ["E", string`"\u{"`], ["E", string`"\u{0"`] ];
string src = lines[lc[0] - 1];,map<TokenizerTestCase> tests = {};
tStart,ParserTestJson[] testData = check (check io:fileReadJson("modules/front.syntax/tests/data/testParser.json")).fromJsonWithType();
int tStart = lc[1];,string[] expected;
tStr,string subject = s[2];
string tStr = tokenToString(t);,string[] subjectLines = splitIntoLines(subject);
srcAtPos,string rule = s[1];
string srcAtPos = src.substring(tStart, tStart + tStr.length());,var testFiles = check file:readDir("modules/front.syntax/tests/data");
e,string path = f.absPath;
err:Syntax? e = tok.advance();,string base = check file:basename(path);
all,string parentDir = check file:parentPath(path);
map<TokenizerTestCase> all = check invalidTokenSourceFragments();,string canonFile = check file:joinPath(parentDir, canonFileName(base));
invalidCases,[Kind, string] baseParts = check splitTestName(base);
int invalidCases = all.length();,int len = base.length();
valid,int kindPos = base.indexOf("-") ?: 0;
map<ParserTestCase> valid = check readParserTests();,string kind = base.substring(0, kindPos);
sources,int afterKindPos = min(kindPos + 1, len);
SingleStringTokenizerTestCase[] sources = [ ["E", string`"`], ["E", "'"], ["E", "`"], ["E", string`"\"`], ["E", string`"\a"`], ["E", string`\`], ["E", string`"${ "\n" }"`], ["E", string`"${ "\r" }"`], ["E", string`"\\`], ["E", string`"\u{}"`], ["E", "\"\\" + "u{D800}\""], ["E", "\"\\" + "u{DFFF}\""], ["E", "\"\\" + "u{110000}\""], ["E", string`"\u{X}"`], ["E", string`"\u{-6A}"`], ["E", string`"\u"`], ["E", string`"\u{"`], ["E", string`"\u{0"`] ];,int rulePos = base.indexOf("-", afterKindPos) ?: afterKindPos;
tests,string[] lines = check io:fileReadLines(path);
map<TokenizerTestCase> tests = {};,string[] caseLines = [];
map<ParserTestCase> tests = {};,boolean inCase = false;
testData,int indented = 0;
ParserTestJson[] testData = check (check io:fileReadJson("modules/front.syntax/tests/data/testParser.json")).fromJsonWithType();,string trimLine = line.trim();
expected,string sansExt = base.substring(0, base.length() - SOURCE_EXTENSION.length());
